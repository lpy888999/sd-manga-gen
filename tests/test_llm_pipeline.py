#!/usr/bin/env python3
"""
LLM Pipeline Test â€” Validate all text stages before diffusion
===============================================================
Tests Steps 0â€“2 (Character Extraction â†’ Story Expansion â†’ Prompt Engineering)
WITHOUT running Stable Diffusion.  Requires only Ollama running locally.

Optional ``--test-tts`` flag also tests Step 2.5 (Script Extraction).

Usage::

    python tests/test_llm_pipeline.py                    # default settings
    python tests/test_llm_pipeline.py --panels 6         # force 6-panel
    python tests/test_llm_pipeline.py --skip-vision      # skip character extraction
    python tests/test_llm_pipeline.py --config config.yaml

Test assets:
    tests/fixtures/reference_character.png   â€” anime character reference image
"""

import argparse
import json
import logging
import sys
import time
from pathlib import Path

# â”€â”€ Setup project path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import yaml
from pipeline.character_extractor import CharacterExtractor
from pipeline.story_expander import StoryExpander
from pipeline.prompt_engineer import PromptEngineer

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FIXTURES_DIR = Path(__file__).resolve().parent / "fixtures"
# DEFAULT_REF_IMAGE = FIXTURES_DIR / "reference_character.png"
DEFAULT_REF_IMAGE = FIXTURES_DIR / "meining.jpg"

# Fixed test prompts â€” deliberately short (â†’ 4 panels) and long (â†’ 6 panels)
TEST_PROMPTS = {
    "short": "This character fights a giant mech robot in a neon-lit city during heavy rain",
    "long": (
        "This character is a bounty hunter who receives a mysterious contract "
        "to retrieve an ancient relic from an abandoned space station. "
        "Along the way he is ambushed by alien parasites and fights them in zero gravity, "
        "only to discover that the relic is actually a sentient sword."
    ),
}


def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s â”‚ %(name)-28s â”‚ %(levelname)-5s â”‚ %(message)s",
        datefmt="%H:%M:%S",
    )


def load_config(config_path: str) -> dict:
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def divider(title: str):
    width = 70
    print(f"\n{'â•' * width}")
    print(f"  {title}")
    print(f"{'â•' * width}\n")


def sub_divider(title: str):
    print(f"\n{'â”€' * 50}")
    print(f"  {title}")
    print(f"{'â”€' * 50}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STEP 0 Â· Character Feature Extraction
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def test_character_extraction(config: dict, image_path: str) -> str:
    divider("STEP 0 Â· Character Feature Extraction (Vision LLM)")

    llm_cfg = config.get("llm", {})
    extractor = CharacterExtractor(
        vision_model_name=llm_cfg.get("vision_model_name", "gemma3:12b"),
        temperature=0.3,
    )

    print(f"  ğŸ“· Reference image: {image_path}")
    print(f"  ğŸ¤– Vision model:    {llm_cfg.get('vision_model_name', 'gemma3:12b')}")
    print()

    t0 = time.time()
    tags = extractor.extract(image_path)
    elapsed = time.time() - t0

    print(f"  âœ… Extracted tags ({elapsed:.1f}s):")
    print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  â”‚ {tags}")
    print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    return tags


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STEP 1 Â· Story Expansion
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def test_story_expansion(config: dict, prompt: str, panel_count: int | None) -> list[str]:
    divider("STEP 1 Â· Story Expansion (Narrative Architect)")

    llm_cfg = config.get("llm", {})
    panel_cfg = config.get("panels", {})

    expander = StoryExpander(
        model_name=llm_cfg.get("model_name", "qwen3-coder-next:cloud"),
        temperature=llm_cfg.get("temperature", 0.7),
    )

    print(f"  ğŸ’¬ Input prompt: {prompt}")
    print(f"  ğŸ¤– Model:        {llm_cfg.get('model_name', 'qwen3-coder-next:cloud')}")
    print()

    t0 = time.time()
    panels = expander.expand(
        user_prompt=prompt,
        panel_count=panel_count,
        auto_threshold=panel_cfg.get("auto_threshold", 30),
    )
    elapsed = time.time() - t0

    print(f"  âœ… Generated {len(panels)} panels ({elapsed:.1f}s):")
    for i, desc in enumerate(panels):
        print(f"\n  â”Œâ”€ Panel {i + 1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        # Wrap long text
        for line in _wrap(desc, 60):
            print(f"  â”‚ {line}")
        print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    return panels


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STEP 2 Â· Prompt Engineering
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def test_prompt_engineering(config: dict, panels: list[str], character_tags: str):
    divider("STEP 2 Â· Prompt Engineering (SD Engineer)")

    llm_cfg = config.get("llm", {})
    prompt_cfg = config.get("prompt", {})
    lora_section = config.get("sd", {}).get("lora", {})

    # Build LoRA tags the same way the pipeline does
    lora_tags = []
    for adapter_name in ("character", "style"):
        adapter = lora_section.get(adapter_name, {})
        lora_dir = adapter.get("dir")
        weight = adapter.get("weight", 0.8)
        if lora_dir:
            dir_path = Path(lora_dir)
            if dir_path.is_dir():
                for sf in sorted(dir_path.glob("*.safetensors")):
                    lora_tags.append(f"<lora:{sf.stem}:{weight}>")

    trigger_words = lora_section.get("trigger_words", [])

    engineer = PromptEngineer(
        model_name=llm_cfg.get("model_name", "qwen3-coder-next:cloud"),
        temperature=0.4,
        quality_suffix=prompt_cfg.get("quality_suffix",
                                      "masterpiece, best quality, high resolution, comic style, thick lineart"),
        negative_prompt=prompt_cfg.get("negative_prompt",
                                       "low quality, blurry, distorted face, extra fingers, bad anatomy"),
        lora_tags=lora_tags,
        trigger_words=trigger_words,
    )

    print(f"  ğŸ·ï¸  Character tags: {character_tags}")
    print(f"  ğŸ¤– Model:          {llm_cfg.get('model_name', 'qwen3-coder-next:cloud')}")
    print(f"  ğŸ”— LoRA tags:      {lora_tags or '(none â€” no .safetensors found)'}")
    print(f"  ğŸ“ Trigger words:  {trigger_words or '(none)'}")
    print()

    t0 = time.time()
    panel_prompts = engineer.generate(
        panels=panels,
        character_features=character_tags,
    )
    elapsed = time.time() - t0

    print(f"  âœ… Generated {len(panel_prompts)} SD prompts ({elapsed:.1f}s):")
    for pp in panel_prompts:
        print(f"\n  â”Œâ”€ Panel {pp.panel_number} ({pp.camera_angle}) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        sub_divider("Raw SD Prompt (from LLM)")
        for line in _wrap(pp.sd_prompt, 65):
            print(f"  â”‚ {line}")
        sub_divider("Final Prompt (with LoRA + quality)")
        for line in _wrap(pp.final_prompt, 65):
            print(f"  â”‚ {line}")
        sub_divider("Negative Prompt")
        for line in _wrap(pp.negative_prompt, 65):
            print(f"  â”‚ {line}")
        print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # Also dump as JSON for easy inspection
    divider("FULL JSON OUTPUT (machine-readable)")
    json_out = [
        {
            "panel_number": pp.panel_number,
            "camera_angle": pp.camera_angle,
            "sd_prompt": pp.sd_prompt,
            "final_prompt": pp.final_prompt,
            "negative_prompt": pp.negative_prompt,
        }
        for pp in panel_prompts
    ]
    print(json.dumps(json_out, ensure_ascii=False, indent=2))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STEP 2.5 Â· Script Extraction (TTS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def test_script_extraction(config: dict, panels: list[str]):
    divider("STEP 2.5 Â· Script Extraction (TTS)")

    llm_cfg = config.get("llm", {})
    tts_cfg = config.get("tts", {})

    from pipeline.script_generator import ScriptGenerator

    # Use TTS-specific model if configured, else fallback to main LLM
    script_model = (
        tts_cfg.get("script_model_name")
        or llm_cfg.get("model_name", "qwen3-coder-next:cloud")
    )
    script_temp = tts_cfg.get("script_temperature", 0.5)

    generator = ScriptGenerator(
        model_name=script_model,
        temperature=script_temp,
    )

    print(f"  ğŸ¤– Script model: {script_model} (temp={script_temp})")
    print(f"  ğŸ“„ Panels: {len(panels)}")
    print()

    t0 = time.time()
    scripts = generator.generate(panels)
    elapsed = time.time() - t0

    print(f"  âœ… Extracted scripts for {len(scripts)} panels ({elapsed:.1f}s):")
    for ps in scripts:
        print(f"\n  â”Œâ”€ Panel {ps.panel} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for line in ps.lines:
            icon = "ğŸ™ï¸" if line.role.lower() == "narrator" else "ğŸ’¬"
            print(f"  â”‚ {icon} [{line.role}/{line.gender}]: \"{line.text}\"")
        print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # JSON dump
    divider("SCRIPT JSON (machine-readable)")
    json_out = [
        {
            "panel": ps.panel,
            "lines": [
                {"role": l.role, "text": l.text, "gender": l.gender}
                for l in ps.lines
            ],
        }
        for ps in scripts
    ]
    print(json.dumps(json_out, ensure_ascii=False, indent=2))


# â”€â”€ Utility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _wrap(text: str, width: int) -> list[str]:
    """Simple word-wrapping for terminal display."""
    words = text.split()
    lines, current = [], ""
    for w in words:
        if current and len(current) + len(w) + 1 > width:
            lines.append(current)
            current = w
        else:
            current = f"{current} {w}" if current else w
    if current:
        lines.append(current)
    return lines or [""]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    parser = argparse.ArgumentParser(
        description="Test LLM pipeline stages (no GPU / diffusion required)",
    )
    parser.add_argument("-c", "--config", default="config.yaml", help="Config path")
    parser.add_argument("--panels", type=int, choices=[4, 6], default=None,
                        help="Force panel count")
    parser.add_argument("--prompt", type=str, default=None,
                        help="Override test prompt")
    parser.add_argument("--prompt-key", choices=["short", "long"], default="short",
                        help="Use built-in test prompt (short=4 panels, long=6)")
    parser.add_argument("--skip-vision", action="store_true",
                        help="Skip vision extraction, use hardcoded tags")
    parser.add_argument("--reference", type=str, default=None,
                        help="Custom reference image path")
    parser.add_argument("--test-tts", action="store_true",
                        help="Also test TTS script extraction (Step 2.5)")
    args = parser.parse_args()

    setup_logging()

    config = load_config(args.config)
    prompt = args.prompt or TEST_PROMPTS[args.prompt_key]
    ref_image = args.reference or str(DEFAULT_REF_IMAGE)

    print()
    print("ğŸ¨ SDXL Manga Generator â€” LLM Pipeline Test")
    print("=" * 50)
    print(f"  Config:    {args.config}")
    print(f"  Prompt:    {prompt}")
    print(f"  Panels:    {args.panels or 'auto'}")
    print(f"  Reference: {ref_image}")
    print(f"  Vision:    {'skip' if args.skip_vision else 'enabled'}")
    print(f"  TTS test:  {'yes' if args.test_tts else 'no'}")
    print("=" * 50)

    total_t0 = time.time()

    # â”€â”€ Step 0: Character tags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.skip_vision:
        character_tags = "1boy, spiky silver hair, sharp blue eyes, long black trench coat, silver buttons, red scarf, dark combat boots"
        divider("STEP 0 Â· Character Tags (manual â€” vision skipped)")
        print(f"  {character_tags}")
    else:
        character_tags = test_character_extraction(config, ref_image)

    # â”€â”€ Step 1: Story expansion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    panels = test_story_expansion(config, prompt, args.panels)

    # â”€â”€ Step 2: Prompt engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    test_prompt_engineering(config, panels, character_tags)

    # â”€â”€ Step 2.5: TTS script extraction (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.test_tts:
        test_script_extraction(config, panels)

    # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elapsed = time.time() - total_t0
    divider(f"ALL DONE â€” {elapsed:.1f}s total")
    print("  Next step: feed the final_prompt values into SD generator")
    print("  (requires GPU + diffusion models)")
    if args.test_tts:
        print("  TTS script was extracted â€” review the dialogue above.")
    print()


if __name__ == "__main__":
    main()
